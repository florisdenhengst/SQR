{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd30e817",
   "metadata": {},
   "source": [
    "# Analysis of SQR Results Across Tau Levels with TikZ Plotting\n",
    "Analysis of `results_sampled_10k_taus` directory with visualization using TikZ for LaTeX embedding. This notebook aggregates results across multiple quantile levels (τ) and generates publication-quality plots with dual y-axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07cd5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: results_sampled_10k_taus/\n",
      "Output directory: tikz_plots/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "RESULTS_DIR = 'results_sampled_10k_taus/'\n",
    "OUTPUT_DIR = 'tikz_plots/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4620dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    \"1027_ESL\",\n",
    "    \"1028_SWD\",\n",
    "    \"1029_LEV\",\n",
    "    \"1030_ERA\",\n",
    "    \"1089_USCrime\",\n",
    "    \"1096_FacultySalaries\",\n",
    "    \"1191_BNG_pbc\",\n",
    "    \"1193_BNG_lowbwt\",\n",
    "    \"1196_BNG_pharynx\",\n",
    "    \"1199_BNG_echoMonths\",\n",
    "    \"1201_BNG_breastTumor\",\n",
    "    \"192_vineyard\",\n",
    "    \"195_auto_price\",\n",
    "    \"197_cpu_act\",\n",
    "    \"201_pol\",\n",
    "    \"207_autoPrice\",\n",
    "    \"210_cloud\",\n",
    "    \"215_2dplanes\",\n",
    "    \"218_house_8L\",\n",
    "    \"225_puma8NH\",\n",
    "    \"227_cpu_small\",\n",
    "    \"228_elusage\",\n",
    "    \"229_pwLinear\",\n",
    "    \"230_machine_cpu\",\n",
    "    \"294_satellite_image\",\n",
    "    \"344_mv\",\n",
    "    \"4544_GeographicalOriginalofMusic\",\n",
    "    \"485_analcatdata_vehicle\",\n",
    "    \"503_wind\",\n",
    "    \"505_tecator\",\n",
    "    \"519_vinnie\",\n",
    "    \"522_pm10\",\n",
    "    \"523_analcatdata_neavote\",\n",
    "    \"527_analcatdata_election2000\",\n",
    "    \"529_pollen\",\n",
    "    \"537_houses\",\n",
    "    \"542_pollution\",\n",
    "    \"547_no2\",\n",
    "    \"556_analcatdata_apnea2\",\n",
    "    \"557_analcatdata_apnea1\",\n",
    "    \"560_bodyfat\",\n",
    "    \"561_cpu\",\n",
    "    \"562_cpu_small\",\n",
    "    \"564_fried\",\n",
    "    \"573_cpu_act\",\n",
    "    \"574_house_16H\",\n",
    "    \"579_fri_c0_250_5\",\n",
    "    \"581_fri_c3_500_25\",\n",
    "    \"582_fri_c1_500_25\",\n",
    "    \"583_fri_c1_1000_50\",\n",
    "    \"584_fri_c4_500_25\",\n",
    "    \"586_fri_c3_1000_25\",\n",
    "    \"588_fri_c4_1000_100\",\n",
    "    \"589_fri_c2_1000_25\",\n",
    "    \"590_fri_c0_1000_50\",\n",
    "    \"591_fri_c1_100_10\",\n",
    "    \"592_fri_c4_1000_25\",\n",
    "    \"593_fri_c1_1000_10\",\n",
    "    \"594_fri_c2_100_5\",\n",
    "    \"595_fri_c0_1000_10\",\n",
    "    \"596_fri_c2_250_5\",\n",
    "    \"597_fri_c2_500_5\",\n",
    "    \"598_fri_c0_1000_25\",\n",
    "    \"599_fri_c2_1000_5\",\n",
    "    \"601_fri_c1_250_5\",\n",
    "    \"602_fri_c3_250_10\",\n",
    "    \"603_fri_c0_250_50\",\n",
    "    \"604_fri_c4_500_10\",\n",
    "    \"605_fri_c2_250_25\",\n",
    "    \"606_fri_c2_1000_10\",\n",
    "    \"607_fri_c4_1000_50\",\n",
    "    \"608_fri_c3_1000_10\",\n",
    "    \"609_fri_c0_1000_5\",\n",
    "    \"611_fri_c3_100_5\",\n",
    "    \"612_fri_c1_1000_5\",\n",
    "    \"613_fri_c3_250_5\",\n",
    "    \"615_fri_c4_250_10\",\n",
    "    \"616_fri_c4_500_50\",\n",
    "    \"617_fri_c3_500_5\",\n",
    "    \"618_fri_c3_1000_50\",\n",
    "    \"620_fri_c1_1000_25\",\n",
    "    \"621_fri_c0_100_10\",\n",
    "    \"622_fri_c2_1000_50\",\n",
    "    \"623_fri_c4_1000_10\",\n",
    "    \"624_fri_c0_100_5\",\n",
    "    \"626_fri_c2_500_50\",\n",
    "    \"627_fri_c2_500_10\",\n",
    "    \"628_fri_c3_1000_5\",\n",
    "    \"631_fri_c1_500_5\",\n",
    "    \"633_fri_c0_500_25\",\n",
    "    \"634_fri_c2_100_10\",\n",
    "    \"635_fri_c0_250_10\",\n",
    "    \"637_fri_c1_500_50\",\n",
    "    \"641_fri_c1_500_10\",\n",
    "    \"643_fri_c2_500_25\",\n",
    "    \"644_fri_c4_250_25\",\n",
    "    \"645_fri_c3_500_50\",\n",
    "    \"646_fri_c3_500_10\",\n",
    "    \"647_fri_c1_250_10\",\n",
    "    \"648_fri_c1_250_50\",\n",
    "    \"649_fri_c0_500_5\",\n",
    "    \"650_fri_c0_500_50\",\n",
    "    \"651_fri_c0_100_25\",\n",
    "    \"653_fri_c0_250_25\",\n",
    "    \"654_fri_c0_500_10\",\n",
    "    \"656_fri_c1_100_5\",\n",
    "    \"657_fri_c2_250_10\",\n",
    "    \"658_fri_c3_250_25\",\n",
    "    \"659_sleuth_ex1714\",\n",
    "    \"663_rabe_266\",\n",
    "    \"665_sleuth_case2002\",\n",
    "    \"666_rmftsa_ladata\",\n",
    "    \"678_visualizing_environmental\",\n",
    "    \"687_sleuth_ex1605\",\n",
    "    \"690_visualizing_galaxy\",\n",
    "    \"695_chatfield_4\",\n",
    "    \"706_sleuth_case1202\",\n",
    "    \"712_chscase_geyser1\",\n",
    "    \"banana\",\n",
    "    \"titanic\"\n",
    "]\n",
    "TAUS = [0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b36bdd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: results_sampled_10k_taus/results0.1_1027_ESL_tau0.1.json for tau=0.1 and dataset=1027_ESL\n",
      "File not found: results_sampled_10k_taus/results0.1_1028_SWD_tau0.1.json for tau=0.1 and dataset=1028_SWD\n",
      "File not found: results_sampled_10k_taus/results0.1_1029_LEV_tau0.1.json for tau=0.1 and dataset=1029_LEV\n",
      "File not found: results_sampled_10k_taus/results0.1_1030_ERA_tau0.1.json for tau=0.1 and dataset=1030_ERA\n",
      "File not found: results_sampled_10k_taus/results0.1_1089_USCrime_tau0.1.json for tau=0.1 and dataset=1089_USCrime\n",
      "File not found: results_sampled_10k_taus/results0.1_1096_FacultySalaries_tau0.1.json for tau=0.1 and dataset=1096_FacultySalaries\n",
      "File not found: results_sampled_10k_taus/results0.1_1191_BNG_pbc_tau0.1.json for tau=0.1 and dataset=1191_BNG_pbc\n",
      "File not found: results_sampled_10k_taus/results0.1_1193_BNG_lowbwt_tau0.1.json for tau=0.1 and dataset=1193_BNG_lowbwt\n",
      "File not found: results_sampled_10k_taus/results0.1_1196_BNG_pharynx_tau0.1.json for tau=0.1 and dataset=1196_BNG_pharynx\n",
      "File not found: results_sampled_10k_taus/results0.1_1199_BNG_echoMonths_tau0.1.json for tau=0.1 and dataset=1199_BNG_echoMonths\n",
      "File not found: results_sampled_10k_taus/results0.1_1201_BNG_breastTumor_tau0.1.json for tau=0.1 and dataset=1201_BNG_breastTumor\n",
      "File not found: results_sampled_10k_taus/results0.1_192_vineyard_tau0.1.json for tau=0.1 and dataset=192_vineyard\n",
      "File not found: results_sampled_10k_taus/results0.1_195_auto_price_tau0.1.json for tau=0.1 and dataset=195_auto_price\n",
      "File not found: results_sampled_10k_taus/results0.1_197_cpu_act_tau0.1.json for tau=0.1 and dataset=197_cpu_act\n",
      "File not found: results_sampled_10k_taus/results0.1_201_pol_tau0.1.json for tau=0.1 and dataset=201_pol\n",
      "File not found: results_sampled_10k_taus/results0.1_207_autoPrice_tau0.1.json for tau=0.1 and dataset=207_autoPrice\n",
      "File not found: results_sampled_10k_taus/results0.1_210_cloud_tau0.1.json for tau=0.1 and dataset=210_cloud\n",
      "File not found: results_sampled_10k_taus/results0.1_215_2dplanes_tau0.1.json for tau=0.1 and dataset=215_2dplanes\n",
      "File not found: results_sampled_10k_taus/results0.1_218_house_8L_tau0.1.json for tau=0.1 and dataset=218_house_8L\n",
      "File not found: results_sampled_10k_taus/results0.1_225_puma8NH_tau0.1.json for tau=0.1 and dataset=225_puma8NH\n",
      "File not found: results_sampled_10k_taus/results0.1_227_cpu_small_tau0.1.json for tau=0.1 and dataset=227_cpu_small\n",
      "File not found: results_sampled_10k_taus/results0.1_228_elusage_tau0.1.json for tau=0.1 and dataset=228_elusage\n",
      "File not found: results_sampled_10k_taus/results0.1_229_pwLinear_tau0.1.json for tau=0.1 and dataset=229_pwLinear\n",
      "File not found: results_sampled_10k_taus/results0.1_230_machine_cpu_tau0.1.json for tau=0.1 and dataset=230_machine_cpu\n",
      "File not found: results_sampled_10k_taus/results0.1_294_satellite_image_tau0.1.json for tau=0.1 and dataset=294_satellite_image\n",
      "File not found: results_sampled_10k_taus/results0.1_344_mv_tau0.1.json for tau=0.1 and dataset=344_mv\n",
      "File not found: results_sampled_10k_taus/results0.1_4544_GeographicalOriginalofMusic_tau0.1.json for tau=0.1 and dataset=4544_GeographicalOriginalofMusic\n",
      "File not found: results_sampled_10k_taus/results0.1_485_analcatdata_vehicle_tau0.1.json for tau=0.1 and dataset=485_analcatdata_vehicle\n",
      "File not found: results_sampled_10k_taus/results0.1_503_wind_tau0.1.json for tau=0.1 and dataset=503_wind\n",
      "File not found: results_sampled_10k_taus/results0.1_505_tecator_tau0.1.json for tau=0.1 and dataset=505_tecator\n",
      "File not found: results_sampled_10k_taus/results0.1_519_vinnie_tau0.1.json for tau=0.1 and dataset=519_vinnie\n",
      "File not found: results_sampled_10k_taus/results0.1_522_pm10_tau0.1.json for tau=0.1 and dataset=522_pm10\n",
      "File not found: results_sampled_10k_taus/results0.1_523_analcatdata_neavote_tau0.1.json for tau=0.1 and dataset=523_analcatdata_neavote\n",
      "File not found: results_sampled_10k_taus/results0.1_527_analcatdata_election2000_tau0.1.json for tau=0.1 and dataset=527_analcatdata_election2000\n",
      "File not found: results_sampled_10k_taus/results0.1_529_pollen_tau0.1.json for tau=0.1 and dataset=529_pollen\n",
      "File not found: results_sampled_10k_taus/results0.1_537_houses_tau0.1.json for tau=0.1 and dataset=537_houses\n",
      "File not found: results_sampled_10k_taus/results0.1_542_pollution_tau0.1.json for tau=0.1 and dataset=542_pollution\n",
      "File not found: results_sampled_10k_taus/results0.1_547_no2_tau0.1.json for tau=0.1 and dataset=547_no2\n",
      "File not found: results_sampled_10k_taus/results0.1_556_analcatdata_apnea2_tau0.1.json for tau=0.1 and dataset=556_analcatdata_apnea2\n",
      "File not found: results_sampled_10k_taus/results0.1_557_analcatdata_apnea1_tau0.1.json for tau=0.1 and dataset=557_analcatdata_apnea1\n",
      "File not found: results_sampled_10k_taus/results0.1_560_bodyfat_tau0.1.json for tau=0.1 and dataset=560_bodyfat\n",
      "File not found: results_sampled_10k_taus/results0.1_561_cpu_tau0.1.json for tau=0.1 and dataset=561_cpu\n",
      "File not found: results_sampled_10k_taus/results0.1_562_cpu_small_tau0.1.json for tau=0.1 and dataset=562_cpu_small\n",
      "File not found: results_sampled_10k_taus/results0.1_564_fried_tau0.1.json for tau=0.1 and dataset=564_fried\n",
      "File not found: results_sampled_10k_taus/results0.1_573_cpu_act_tau0.1.json for tau=0.1 and dataset=573_cpu_act\n",
      "File not found: results_sampled_10k_taus/results0.1_574_house_16H_tau0.1.json for tau=0.1 and dataset=574_house_16H\n",
      "File not found: results_sampled_10k_taus/results0.1_579_fri_c0_250_5_tau0.1.json for tau=0.1 and dataset=579_fri_c0_250_5\n",
      "File not found: results_sampled_10k_taus/results0.1_581_fri_c3_500_25_tau0.1.json for tau=0.1 and dataset=581_fri_c3_500_25\n",
      "File not found: results_sampled_10k_taus/results0.1_582_fri_c1_500_25_tau0.1.json for tau=0.1 and dataset=582_fri_c1_500_25\n",
      "File not found: results_sampled_10k_taus/results0.1_583_fri_c1_1000_50_tau0.1.json for tau=0.1 and dataset=583_fri_c1_1000_50\n",
      "File not found: results_sampled_10k_taus/results0.1_584_fri_c4_500_25_tau0.1.json for tau=0.1 and dataset=584_fri_c4_500_25\n",
      "File not found: results_sampled_10k_taus/results0.1_586_fri_c3_1000_25_tau0.1.json for tau=0.1 and dataset=586_fri_c3_1000_25\n",
      "File not found: results_sampled_10k_taus/results0.1_588_fri_c4_1000_100_tau0.1.json for tau=0.1 and dataset=588_fri_c4_1000_100\n",
      "File not found: results_sampled_10k_taus/results0.1_589_fri_c2_1000_25_tau0.1.json for tau=0.1 and dataset=589_fri_c2_1000_25\n",
      "File not found: results_sampled_10k_taus/results0.1_590_fri_c0_1000_50_tau0.1.json for tau=0.1 and dataset=590_fri_c0_1000_50\n",
      "File not found: results_sampled_10k_taus/results0.1_591_fri_c1_100_10_tau0.1.json for tau=0.1 and dataset=591_fri_c1_100_10\n",
      "File not found: results_sampled_10k_taus/results0.1_592_fri_c4_1000_25_tau0.1.json for tau=0.1 and dataset=592_fri_c4_1000_25\n",
      "File not found: results_sampled_10k_taus/results0.1_593_fri_c1_1000_10_tau0.1.json for tau=0.1 and dataset=593_fri_c1_1000_10\n",
      "File not found: results_sampled_10k_taus/results0.1_594_fri_c2_100_5_tau0.1.json for tau=0.1 and dataset=594_fri_c2_100_5\n",
      "File not found: results_sampled_10k_taus/results0.1_595_fri_c0_1000_10_tau0.1.json for tau=0.1 and dataset=595_fri_c0_1000_10\n",
      "File not found: results_sampled_10k_taus/results0.1_596_fri_c2_250_5_tau0.1.json for tau=0.1 and dataset=596_fri_c2_250_5\n",
      "File not found: results_sampled_10k_taus/results0.1_597_fri_c2_500_5_tau0.1.json for tau=0.1 and dataset=597_fri_c2_500_5\n",
      "File not found: results_sampled_10k_taus/results0.1_598_fri_c0_1000_25_tau0.1.json for tau=0.1 and dataset=598_fri_c0_1000_25\n",
      "File not found: results_sampled_10k_taus/results0.1_599_fri_c2_1000_5_tau0.1.json for tau=0.1 and dataset=599_fri_c2_1000_5\n",
      "File not found: results_sampled_10k_taus/results0.1_601_fri_c1_250_5_tau0.1.json for tau=0.1 and dataset=601_fri_c1_250_5\n",
      "File not found: results_sampled_10k_taus/results0.1_602_fri_c3_250_10_tau0.1.json for tau=0.1 and dataset=602_fri_c3_250_10\n",
      "File not found: results_sampled_10k_taus/results0.1_603_fri_c0_250_50_tau0.1.json for tau=0.1 and dataset=603_fri_c0_250_50\n",
      "File not found: results_sampled_10k_taus/results0.1_604_fri_c4_500_10_tau0.1.json for tau=0.1 and dataset=604_fri_c4_500_10\n",
      "File not found: results_sampled_10k_taus/results0.1_605_fri_c2_250_25_tau0.1.json for tau=0.1 and dataset=605_fri_c2_250_25\n",
      "File not found: results_sampled_10k_taus/results0.1_606_fri_c2_1000_10_tau0.1.json for tau=0.1 and dataset=606_fri_c2_1000_10\n",
      "File not found: results_sampled_10k_taus/results0.1_607_fri_c4_1000_50_tau0.1.json for tau=0.1 and dataset=607_fri_c4_1000_50\n",
      "File not found: results_sampled_10k_taus/results0.1_608_fri_c3_1000_10_tau0.1.json for tau=0.1 and dataset=608_fri_c3_1000_10\n",
      "File not found: results_sampled_10k_taus/results0.1_609_fri_c0_1000_5_tau0.1.json for tau=0.1 and dataset=609_fri_c0_1000_5\n",
      "File not found: results_sampled_10k_taus/results0.1_611_fri_c3_100_5_tau0.1.json for tau=0.1 and dataset=611_fri_c3_100_5\n",
      "File not found: results_sampled_10k_taus/results0.1_612_fri_c1_1000_5_tau0.1.json for tau=0.1 and dataset=612_fri_c1_1000_5\n",
      "File not found: results_sampled_10k_taus/results0.1_613_fri_c3_250_5_tau0.1.json for tau=0.1 and dataset=613_fri_c3_250_5\n",
      "File not found: results_sampled_10k_taus/results0.1_615_fri_c4_250_10_tau0.1.json for tau=0.1 and dataset=615_fri_c4_250_10\n",
      "File not found: results_sampled_10k_taus/results0.1_616_fri_c4_500_50_tau0.1.json for tau=0.1 and dataset=616_fri_c4_500_50\n",
      "File not found: results_sampled_10k_taus/results0.1_617_fri_c3_500_5_tau0.1.json for tau=0.1 and dataset=617_fri_c3_500_5\n",
      "File not found: results_sampled_10k_taus/results0.1_618_fri_c3_1000_50_tau0.1.json for tau=0.1 and dataset=618_fri_c3_1000_50\n",
      "File not found: results_sampled_10k_taus/results0.1_620_fri_c1_1000_25_tau0.1.json for tau=0.1 and dataset=620_fri_c1_1000_25\n",
      "File not found: results_sampled_10k_taus/results0.1_621_fri_c0_100_10_tau0.1.json for tau=0.1 and dataset=621_fri_c0_100_10\n",
      "File not found: results_sampled_10k_taus/results0.1_622_fri_c2_1000_50_tau0.1.json for tau=0.1 and dataset=622_fri_c2_1000_50\n",
      "File not found: results_sampled_10k_taus/results0.1_623_fri_c4_1000_10_tau0.1.json for tau=0.1 and dataset=623_fri_c4_1000_10\n",
      "File not found: results_sampled_10k_taus/results0.1_624_fri_c0_100_5_tau0.1.json for tau=0.1 and dataset=624_fri_c0_100_5\n",
      "File not found: results_sampled_10k_taus/results0.1_626_fri_c2_500_50_tau0.1.json for tau=0.1 and dataset=626_fri_c2_500_50\n",
      "File not found: results_sampled_10k_taus/results0.1_627_fri_c2_500_10_tau0.1.json for tau=0.1 and dataset=627_fri_c2_500_10\n",
      "File not found: results_sampled_10k_taus/results0.1_628_fri_c3_1000_5_tau0.1.json for tau=0.1 and dataset=628_fri_c3_1000_5\n",
      "File not found: results_sampled_10k_taus/results0.1_631_fri_c1_500_5_tau0.1.json for tau=0.1 and dataset=631_fri_c1_500_5\n",
      "File not found: results_sampled_10k_taus/results0.1_633_fri_c0_500_25_tau0.1.json for tau=0.1 and dataset=633_fri_c0_500_25\n",
      "File not found: results_sampled_10k_taus/results0.1_634_fri_c2_100_10_tau0.1.json for tau=0.1 and dataset=634_fri_c2_100_10\n",
      "File not found: results_sampled_10k_taus/results0.1_635_fri_c0_250_10_tau0.1.json for tau=0.1 and dataset=635_fri_c0_250_10\n",
      "File not found: results_sampled_10k_taus/results0.1_637_fri_c1_500_50_tau0.1.json for tau=0.1 and dataset=637_fri_c1_500_50\n",
      "File not found: results_sampled_10k_taus/results0.1_641_fri_c1_500_10_tau0.1.json for tau=0.1 and dataset=641_fri_c1_500_10\n",
      "File not found: results_sampled_10k_taus/results0.1_643_fri_c2_500_25_tau0.1.json for tau=0.1 and dataset=643_fri_c2_500_25\n",
      "File not found: results_sampled_10k_taus/results0.1_644_fri_c4_250_25_tau0.1.json for tau=0.1 and dataset=644_fri_c4_250_25\n",
      "File not found: results_sampled_10k_taus/results0.1_645_fri_c3_500_50_tau0.1.json for tau=0.1 and dataset=645_fri_c3_500_50\n",
      "File not found: results_sampled_10k_taus/results0.1_646_fri_c3_500_10_tau0.1.json for tau=0.1 and dataset=646_fri_c3_500_10\n",
      "File not found: results_sampled_10k_taus/results0.1_647_fri_c1_250_10_tau0.1.json for tau=0.1 and dataset=647_fri_c1_250_10\n",
      "File not found: results_sampled_10k_taus/results0.1_648_fri_c1_250_50_tau0.1.json for tau=0.1 and dataset=648_fri_c1_250_50\n",
      "File not found: results_sampled_10k_taus/results0.1_649_fri_c0_500_5_tau0.1.json for tau=0.1 and dataset=649_fri_c0_500_5\n",
      "File not found: results_sampled_10k_taus/results0.1_650_fri_c0_500_50_tau0.1.json for tau=0.1 and dataset=650_fri_c0_500_50\n",
      "File not found: results_sampled_10k_taus/results0.1_651_fri_c0_100_25_tau0.1.json for tau=0.1 and dataset=651_fri_c0_100_25\n",
      "File not found: results_sampled_10k_taus/results0.1_653_fri_c0_250_25_tau0.1.json for tau=0.1 and dataset=653_fri_c0_250_25\n",
      "File not found: results_sampled_10k_taus/results0.1_654_fri_c0_500_10_tau0.1.json for tau=0.1 and dataset=654_fri_c0_500_10\n",
      "File not found: results_sampled_10k_taus/results0.1_656_fri_c1_100_5_tau0.1.json for tau=0.1 and dataset=656_fri_c1_100_5\n",
      "File not found: results_sampled_10k_taus/results0.1_657_fri_c2_250_10_tau0.1.json for tau=0.1 and dataset=657_fri_c2_250_10\n",
      "File not found: results_sampled_10k_taus/results0.1_658_fri_c3_250_25_tau0.1.json for tau=0.1 and dataset=658_fri_c3_250_25\n",
      "File not found: results_sampled_10k_taus/results0.1_659_sleuth_ex1714_tau0.1.json for tau=0.1 and dataset=659_sleuth_ex1714\n",
      "File not found: results_sampled_10k_taus/results0.1_663_rabe_266_tau0.1.json for tau=0.1 and dataset=663_rabe_266\n",
      "File not found: results_sampled_10k_taus/results0.1_665_sleuth_case2002_tau0.1.json for tau=0.1 and dataset=665_sleuth_case2002\n",
      "File not found: results_sampled_10k_taus/results0.1_666_rmftsa_ladata_tau0.1.json for tau=0.1 and dataset=666_rmftsa_ladata\n",
      "File not found: results_sampled_10k_taus/results0.1_678_visualizing_environmental_tau0.1.json for tau=0.1 and dataset=678_visualizing_environmental\n",
      "File not found: results_sampled_10k_taus/results0.1_687_sleuth_ex1605_tau0.1.json for tau=0.1 and dataset=687_sleuth_ex1605\n",
      "File not found: results_sampled_10k_taus/results0.1_690_visualizing_galaxy_tau0.1.json for tau=0.1 and dataset=690_visualizing_galaxy\n",
      "File not found: results_sampled_10k_taus/results0.1_695_chatfield_4_tau0.1.json for tau=0.1 and dataset=695_chatfield_4\n",
      "File not found: results_sampled_10k_taus/results0.1_706_sleuth_case1202_tau0.1.json for tau=0.1 and dataset=706_sleuth_case1202\n",
      "File not found: results_sampled_10k_taus/results0.1_712_chscase_geyser1_tau0.1.json for tau=0.1 and dataset=712_chscase_geyser1\n",
      "File not found: results_sampled_10k_taus/results0.1_banana_tau0.1.json for tau=0.1 and dataset=banana\n",
      "File not found: results_sampled_10k_taus/results0.1_titanic_tau0.1.json for tau=0.1 and dataset=titanic\n"
     ]
    }
   ],
   "source": [
    "# Check whether results are complete\n",
    "for tau in TAUS:\n",
    "    for ds_name in data_list:\n",
    "        # check whether file exists, print if that's not the case\n",
    "        file_path = Path(RESULTS_DIR) / f\"results{tau}_{ds_name}_tau{tau}.json\"\n",
    "        if not file_path.exists():\n",
    "            print(f\"File not found: {file_path} for tau={tau} and dataset={ds_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d380bb6",
   "metadata": {},
   "source": [
    "## Section 1: Load and Aggregate Results by Tau\n",
    "Load JSON files from results_sampled_10k_taus directory and create a long-format DataFrame with tau as a separate dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac01bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading results0.6_503_wind_tau0.6.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loaded 56905 records from results_sampled_10k_taus/\n",
      "Unique tau values: [np.float64(0.5), np.float64(0.6), np.float64(0.7), np.float64(0.8), np.float64(0.9)]\n",
      "Unique models: ['DecisionTree', 'LightGBM', 'LinearQuantile', 'SQR']\n",
      "Unique metrics: ['complexity', 'coverage', 'losses', 'time_all', 'time_fit']\n",
      "\\nDataFrame shape: (56905, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>model</th>\n",
       "      <th>tau</th>\n",
       "      <th>metric</th>\n",
       "      <th>dataset</th>\n",
       "      <th>run</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.367347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.112245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.283505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.273196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename model  tau    metric   dataset  run  \\\n",
       "0  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    0   \n",
       "1  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    1   \n",
       "2  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    2   \n",
       "3  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    3   \n",
       "4  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    4   \n",
       "5  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    0   \n",
       "6  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    1   \n",
       "7  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    2   \n",
       "8  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    3   \n",
       "9  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    4   \n",
       "\n",
       "      value  \n",
       "0  0.035077  \n",
       "1  0.044006  \n",
       "2  0.034439  \n",
       "3  0.034149  \n",
       "4  0.038015  \n",
       "5  0.367347  \n",
       "6  0.112245  \n",
       "7  0.397959  \n",
       "8  0.283505  \n",
       "9  0.273196  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tau_jsons_to_long_df(directory):\n",
    "    \"\"\"Load JSON files from tau results directory into long-format DataFrame.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                for model, model_data in data.items():\n",
    "                    tau = model_data.get('tau')\n",
    "                    for metric, metric_data in model_data.items():\n",
    "                        if metric == 'tau' or metric == 'sizes':\n",
    "                            continue\n",
    "                        for dataset, values in metric_data.items():\n",
    "                            for run_idx, value in enumerate(values):\n",
    "                                records.append({\n",
    "                                    'filename': filename,\n",
    "                                    'model': model,\n",
    "                                    'tau': tau,\n",
    "                                    'metric': metric,\n",
    "                                    'dataset': dataset,\n",
    "                                    'run': run_idx,\n",
    "                                    'value': value\n",
    "                                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    return df\n",
    "\n",
    "# Load results\n",
    "df_long = load_tau_jsons_to_long_df(RESULTS_DIR)\n",
    "print(f\"Loaded {len(df_long)} records from {RESULTS_DIR}\")\n",
    "print(f\"Unique tau values: {sorted(df_long['tau'].unique())}\")\n",
    "print(f\"Unique models: {sorted(df_long['model'].unique())}\")\n",
    "print(f\"Unique metrics: {sorted(df_long['metric'].unique())}\")\n",
    "print(f\"\\\\nDataFrame shape: {df_long.shape}\")\n",
    "df_long.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe8cb8",
   "metadata": {},
   "source": [
    "## Section 2: Calculate Summary Statistics\n",
    "Group data by tau, model, and metric to compute mean, median, and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17af7626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics shape: (95, 9)\n",
      "\\nSample statistics:\n",
      "    tau           model      metric         mean          std     median  \\\n",
      "0   0.5    DecisionTree  complexity  1526.940000  9135.109392  79.000000   \n",
      "1   0.5    DecisionTree    coverage     0.083617     0.099913   0.046872   \n",
      "2   0.5    DecisionTree      losses     0.038717     0.017254   0.038650   \n",
      "3   0.5    DecisionTree    time_all     7.856714    47.333635   0.096175   \n",
      "4   0.5    DecisionTree    time_fit     0.526921     3.229699   0.014585   \n",
      "5   0.5        LightGBM    coverage     0.058731     0.062022   0.040000   \n",
      "6   0.5        LightGBM      losses     0.042091     0.021733   0.036913   \n",
      "7   0.5        LightGBM    time_all     0.603711     1.052114   0.255252   \n",
      "8   0.5        LightGBM    time_fit     0.117287     0.215799   0.053976   \n",
      "9   0.5  LinearQuantile  complexity    19.891667    20.600256  11.000000   \n",
      "10  0.5  LinearQuantile    coverage     0.059011     0.073643   0.040000   \n",
      "11  0.5  LinearQuantile      losses     0.057968     0.029411   0.058179   \n",
      "12  0.5  LinearQuantile    time_all    71.279094   495.958995   0.732135   \n",
      "13  0.5  LinearQuantile    time_fit     6.012549    48.475666   0.058632   \n",
      "14  0.5             SQR  complexity    10.101667     4.500936  11.000000   \n",
      "\n",
      "             min            max  count  \n",
      "0   1.000000e+00  128943.000000    600  \n",
      "1   0.000000e+00       0.446226    600  \n",
      "2   9.759806e-04       0.107143    600  \n",
      "3   2.416039e-02     412.072100    600  \n",
      "4   1.440763e-03      31.157656    600  \n",
      "5   0.000000e+00       0.400000    600  \n",
      "6   4.062980e-03       0.150808    600  \n",
      "7   3.056002e-02       7.591570    600  \n",
      "8   3.676414e-03       1.609080    600  \n",
      "9   2.000000e+00     124.000000    600  \n",
      "10  0.000000e+00       0.500000    600  \n",
      "11  4.652457e-14       0.230189    600  \n",
      "12  2.730465e-02    5742.128815    600  \n",
      "13  1.907349e-03     776.256346    600  \n",
      "14  2.000000e+00      21.000000    600  \n",
      "\\nPivoted statistics (means):\n",
      "metric  tau           model   complexity  coverage    losses    time_all  \\\n",
      "0       0.5    DecisionTree  1526.940000  0.083617  0.038717    7.856714   \n",
      "1       0.5        LightGBM          NaN  0.058731  0.042091    0.603711   \n",
      "2       0.5  LinearQuantile    19.891667  0.059011  0.057968   71.279094   \n",
      "3       0.5             SQR    10.101667  0.081545  0.032786  165.855329   \n",
      "4       0.6    DecisionTree  1307.157983  0.082674  0.038134    7.842902   \n",
      "5       0.6        LightGBM          NaN  0.062003  0.043783    0.612140   \n",
      "6       0.6  LinearQuantile    19.941176  0.106894  0.058978   26.663235   \n",
      "7       0.6             SQR     9.843697  0.079169  0.031723  164.835667   \n",
      "8       0.7    DecisionTree  1050.726667  0.087250  0.035276    7.759449   \n",
      "9       0.7        LightGBM          NaN  0.059984  0.041879    0.606054   \n",
      "10      0.7  LinearQuantile    19.891667  0.162340  0.058438   28.914503   \n",
      "11      0.7             SQR     9.598333  0.073303  0.029593  165.321588   \n",
      "12      0.8    DecisionTree   976.816667  0.083559  0.029495    8.111070   \n",
      "13      0.8        LightGBM          NaN  0.056286  0.036403    0.568040   \n",
      "14      0.8  LinearQuantile    19.891667  0.218424  0.056426   28.327441   \n",
      "15      0.8             SQR     9.735000  0.067302  0.027207  165.804617   \n",
      "16      0.9    DecisionTree   938.870000  0.063960  0.019622    7.804577   \n",
      "17      0.9        LightGBM          NaN  0.038269  0.024369    0.532874   \n",
      "18      0.9  LinearQuantile    19.891667  0.272719  0.052025   46.197629   \n",
      "19      0.9             SQR     9.678333  0.059893       inf  166.141980   \n",
      "\n",
      "metric    time_fit  \n",
      "0         0.526921  \n",
      "1         0.117287  \n",
      "2         6.012549  \n",
      "3       165.855329  \n",
      "4         0.484528  \n",
      "5         0.119900  \n",
      "6         2.438704  \n",
      "7       164.835667  \n",
      "8         0.429903  \n",
      "9         0.117840  \n",
      "10        2.612462  \n",
      "11      165.321588  \n",
      "12        0.439787  \n",
      "13        0.113884  \n",
      "14        2.476702  \n",
      "15      165.804617  \n",
      "16        0.422117  \n",
      "17        0.101827  \n",
      "18        4.217849  \n",
      "19      166.141980  \n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics grouped by tau, model, and metric\n",
    "summary_stats = df_long.groupby(['tau', 'model', 'metric'])['value'].agg([\n",
    "    ('mean', 'mean'),\n",
    "    ('std', 'std'),\n",
    "    ('median', 'median'),\n",
    "    ('min', 'min'),\n",
    "    ('max', 'max'),\n",
    "    ('count', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "print(\"Summary statistics shape:\", summary_stats.shape)\n",
    "print(\"\\\\nSample statistics:\")\n",
    "print(summary_stats.head(15))\n",
    "\n",
    "# Pivot to get metrics as columns\n",
    "stats_pivot = summary_stats.pivot_table(\n",
    "    index=['tau', 'model'],\n",
    "    columns='metric',\n",
    "    values='mean'\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\\\nPivoted statistics (means):\")\n",
    "print(stats_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af17e71",
   "metadata": {},
   "source": [
    "## Section 3: Prepare Data for Plotting\n",
    "Organize metrics into groups for dual-axis visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1abcf403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['DecisionTree', 'LightGBM', 'LinearQuantile', 'SQR']\n",
      "Metrics: ['complexity', 'coverage', 'losses', 'time_all', 'time_fit']\n",
      "Tau values: [np.float64(0.5), np.float64(0.6), np.float64(0.7), np.float64(0.8), np.float64(0.9)]\n",
      "\\nExample: Losses across tau levels\n",
      "    tau           model      mean       std\n",
      "2   0.5    DecisionTree  0.038717  0.017254\n",
      "6   0.5        LightGBM  0.042091  0.021733\n",
      "11  0.5  LinearQuantile  0.057968  0.029411\n",
      "16  0.5             SQR  0.032786  0.023446\n",
      "21  0.6    DecisionTree  0.038134  0.017316\n",
      "25  0.6        LightGBM  0.043783  0.029528\n",
      "30  0.6  LinearQuantile  0.058978  0.032384\n",
      "35  0.6             SQR  0.031723  0.022169\n",
      "54  0.7             SQR  0.029593  0.022415\n",
      "49  0.7  LinearQuantile  0.058438  0.035775\n",
      "44  0.7        LightGBM  0.041879  0.027761\n",
      "40  0.7    DecisionTree  0.035276  0.016283\n",
      "59  0.8    DecisionTree  0.029495  0.013844\n",
      "63  0.8        LightGBM  0.036403  0.022113\n",
      "68  0.8  LinearQuantile  0.056426  0.038815\n",
      "73  0.8             SQR  0.027207  0.063031\n",
      "78  0.9    DecisionTree  0.019622  0.009453\n",
      "82  0.9        LightGBM  0.024369  0.012036\n",
      "87  0.9  LinearQuantile  0.052025  0.043581\n",
      "92  0.9             SQR       inf       NaN\n"
     ]
    }
   ],
   "source": [
    "# Create tau-ordered data for each model and metric\n",
    "models = sorted(df_long['model'].unique())\n",
    "metrics = sorted(df_long['metric'].unique())\n",
    "taus = sorted(df_long['tau'].unique())\n",
    "\n",
    "print(f\"Models: {models}\")\n",
    "print(f\"Metrics: {metrics}\")\n",
    "print(f\"Tau values: {taus}\")\n",
    "\n",
    "# Create a dictionary of DataFrames organized by metric\n",
    "metric_data = {}\n",
    "for metric in metrics:\n",
    "    metric_subset = summary_stats[summary_stats['metric'] == metric].copy()\n",
    "    metric_subset = metric_subset.sort_values('tau')\n",
    "    metric_data[metric] = metric_subset\n",
    "\n",
    "print(\"\\\\nExample: Losses across tau levels\")\n",
    "print(metric_data['losses'][['tau', 'model', 'mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff75871",
   "metadata": {},
   "source": [
    "## Section 4: Generate TikZ Plots with Dual Axes\n",
    "Create TikZ code with dual y-axes (left for one metric, right for another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "822491af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated TikZ code (first 400 chars):\n",
      "\\begin{tikzpicture}\\n\\begin{axis}[\\n    title={Model Performance vs Tau},\\n    xlabel={$\\\\tau$},\\n    ylabel={Normalized Pinball Loss},\\n    width=0.9\\\\textwidth,\\n    height=6cm,\\n    legend pos=outer north east,\\n    xtick={0.5,0.6,0.7,0.8,0.9},\\n]\\n\\\\addplot[color=green, dotted, mark=square, mark size=2pt] coordinates {(0.5,0.0387) (0.6,0.0381) (0.7,0.0353) (0.8,0.0295) (0.9,0.0196)};\\n\\\\addleg\n"
     ]
    }
   ],
   "source": [
    "def generate_tikz_dual_axis_plot(metric_data, left_metrics, right_metrics, models, taus, title=\"\", y_label_left=\"\", y_label_right=\"\"):\n",
    "    \"\"\"Generate TikZ code for a dual-axis plot with tau on x-axis.\"\"\"\n",
    "    \n",
    "    colors = {'SQR': 'red', 'LightGBM': 'blue', 'DecisionTree': 'green', 'LinearQuantile': 'purple'}\n",
    "    line_styles = {'SQR': 'solid', 'LightGBM': 'dashed', 'DecisionTree': 'dotted', 'LinearQuantile': 'dashdotted'}\n",
    "    mark_styles = {'SQR': '*', 'LightGBM': 'o', 'DecisionTree': 'square', 'LinearQuantile': 'diamond'}\n",
    "    \n",
    "    tikz_code = r\"\\begin{tikzpicture}\" + \"\\\\n\"\n",
    "    tikz_code += r\"\\begin{axis}[\" + \"\\\\n\"\n",
    "    tikz_code += f\"    title={{{title}}},\\\\n\"\n",
    "    tikz_code += f\"    xlabel={{$\\\\\\\\tau$}},\\\\n\"\n",
    "    tikz_code += f\"    ylabel={{{y_label_left}}},\\\\n\"\n",
    "    tikz_code += f\"    width=0.9\\\\\\\\textwidth,\\\\n\"\n",
    "    tikz_code += f\"    height=6cm,\\\\n\"\n",
    "    tikz_code += f\"    legend pos=outer north east,\\\\n\"\n",
    "    tikz_code += f\"    xtick={{{','.join(map(str, taus))}}},\\\\n\"\n",
    "    tikz_code += f\"]\\\\n\"\n",
    "    \n",
    "    # Plot left axis metrics\n",
    "    for metric in left_metrics:\n",
    "        if metric not in metric_data:\n",
    "            continue\n",
    "        metric_df = metric_data[metric]\n",
    "        \n",
    "        for model in models:\n",
    "            model_subset = metric_df[metric_df['model'] == model].sort_values('tau')\n",
    "            if len(model_subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            x_vals = model_subset['tau'].tolist()\n",
    "            y_vals = model_subset['mean'].tolist()\n",
    "            coords = ' '.join([f\"({x},{y:.4f})\" for x, y in zip(x_vals, y_vals)])\n",
    "            \n",
    "            tikz_code += f\"\\\\\\\\addplot[color={colors.get(model, 'black')}, {line_styles.get(model, 'solid')}, mark={mark_styles.get(model, 'o')}, mark size=2pt] coordinates {{{coords}}};\\\\n\"\n",
    "            tikz_code += f\"\\\\\\\\addlegendentry{{{model} - {metric}}}\\\\n\"\n",
    "    \n",
    "    tikz_code += r\"\\end{axis}\" + \"\\\\n\"\n",
    "    tikz_code += r\"\\end{tikzpicture}\" + \"\\\\n\"\n",
    "    \n",
    "    return tikz_code\n",
    "\n",
    "test_tikz = generate_tikz_dual_axis_plot(\n",
    "    metric_data,\n",
    "    left_metrics=['losses'],\n",
    "    right_metrics=[],\n",
    "    models=models,\n",
    "    taus=taus,\n",
    "    title=\"Model Performance vs Tau\",\n",
    "    y_label_left=\"Normalized Pinball Loss\"\n",
    ")\n",
    "\n",
    "print(\"Generated TikZ code (first 400 chars):\")\n",
    "print(test_tikz[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b93c12",
   "metadata": {},
   "source": [
    "## Section 5: Export TikZ Figures for LaTeX\n",
    "Save TikZ files ready for inclusion in LaTeX documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c5a0411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TikZ: tikz_plots/losses.tikz\n",
      "Saved TikZ: tikz_plots/coverage.tikz\n",
      "Saved TikZ: tikz_plots/timing.tikz\n",
      "\\n✓ Generated TikZ plots in tikz_plots//\n"
     ]
    }
   ],
   "source": [
    "def save_tikz_standalone(tikz_code, filename, directory='tikz_plots/'):\n",
    "    \"\"\"Save TikZ code as a file for LaTeX inclusion.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    tikz_path = os.path.join(directory, f\"{filename}.tikz\")\n",
    "    with open(tikz_path, 'w') as f:\n",
    "        f.write(tikz_code)\n",
    "    print(f\"Saved TikZ: {tikz_path}\")\n",
    "\n",
    "# Generate and save plots\n",
    "plot_configs = [\n",
    "    {'name': 'losses', 'left_metrics': ['losses'], 'right_metrics': [], 'title': 'Loss vs Tau'},\n",
    "    {'name': 'coverage', 'left_metrics': ['coverage'], 'right_metrics': [], 'title': 'Coverage Error vs Tau'},\n",
    "    {'name': 'timing', 'left_metrics': ['time_all'], 'right_metrics': [], 'title': 'Computation Time vs Tau'}\n",
    "]\n",
    "\n",
    "for config in plot_configs:\n",
    "    tikz_code = generate_tikz_dual_axis_plot(\n",
    "        metric_data,\n",
    "        left_metrics=config['left_metrics'],\n",
    "        right_metrics=config['right_metrics'],\n",
    "        models=models,\n",
    "        taus=taus,\n",
    "        title=config['title'],\n",
    "        y_label_left=config['name']\n",
    "    )\n",
    "    save_tikz_standalone(tikz_code, config['name'])\n",
    "\n",
    "print(f\"\\\\n✓ Generated TikZ plots in {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8c3df",
   "metadata": {},
   "source": [
    "## Summary: Key Results\n",
    "Export summary statistics and model rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd1137df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: tikz_plots/summary_statistics.csv\n",
      "\\n============================================================\n",
      "BEST MODELS BY METRIC AND TAU\n",
      "============================================================\n",
      "\\nLOSSES:\n",
      "  τ=0.5: SQR             (loss=0.03279)\n",
      "  τ=0.6: SQR             (loss=0.03172)\n",
      "  τ=0.7: SQR             (loss=0.02959)\n",
      "  τ=0.8: SQR             (loss=0.02721)\n",
      "  τ=0.9: DecisionTree    (loss=0.01962)\n",
      "\\nCOVERAGE:\n",
      "  τ=0.5: LightGBM        (loss=0.05873)\n",
      "  τ=0.6: LightGBM        (loss=0.06200)\n",
      "  τ=0.7: LightGBM        (loss=0.05998)\n",
      "  τ=0.8: LightGBM        (loss=0.05629)\n",
      "  τ=0.9: LightGBM        (loss=0.03827)\n",
      "\\n✓ Analysis complete. Results in tikz_plots//\n"
     ]
    }
   ],
   "source": [
    "# Export summary statistics to CSV\n",
    "summary_csv_path = os.path.join(OUTPUT_DIR, 'summary_statistics.csv')\n",
    "summary_stats.to_csv(summary_csv_path, index=False)\n",
    "print(f\"Exported: {summary_csv_path}\")\n",
    "\n",
    "# Best models by metric and tau\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"BEST MODELS BY METRIC AND TAU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric in ['losses', 'coverage']:\n",
    "    print(f\"\\\\n{metric.upper()}:\")\n",
    "    metric_data_full = summary_stats[summary_stats['metric'] == metric]\n",
    "    \n",
    "    for tau in taus:\n",
    "        tau_metric = metric_data_full[metric_data_full['tau'] == tau]\n",
    "        best_idx = tau_metric['mean'].idxmin()\n",
    "        best_model = tau_metric.loc[best_idx, 'model']\n",
    "        best_value = tau_metric.loc[best_idx, 'mean']\n",
    "        print(f\"  τ={tau}: {best_model:15s} (loss={best_value:.5f})\")\n",
    "\n",
    "print(f\"\\\\n✓ Analysis complete. Results in {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bf5cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TikZ: tikz_plots/pgfplots_losses.tikz\n",
      "✓ Generated advanced PGFPlots visualization\n",
      "  File: tikz_plots//pgfplots_losses.tikz\n",
      "\n",
      "================================================================================\n",
      "LOSS TREND ANALYSIS ACROSS TAU\n",
      "================================================================================\n",
      "\n",
      "Normalized Pinball Loss by Model and Tau:\n",
      "tau                0.5     0.6     0.7     0.8     0.9\n",
      "model                                                 \n",
      "DecisionTree    0.0387  0.0381  0.0353  0.0295  0.0196\n",
      "LightGBM        0.0421  0.0438  0.0419  0.0364  0.0244\n",
      "LinearQuantile  0.0580  0.0590  0.0584  0.0564  0.0520\n",
      "SQR             0.0328  0.0317  0.0296  0.0272     inf\n",
      "\n",
      "Trend Analysis (from τ=0.5 to τ=0.9):\n",
      "                 tau_min   tau_max     delta  percent_change\n",
      "model                                                       \n",
      "DecisionTree    0.038717  0.019622 -0.019095          -49.32\n",
      "LightGBM        0.042091  0.024369 -0.017721          -42.10\n",
      "LinearQuantile  0.057968  0.052025 -0.005943          -10.25\n",
      "SQR             0.032786       inf       inf             inf\n"
     ]
    }
   ],
   "source": [
    "def generate_pgfplots_dual_axis(metric_data, left_metrics, right_metrics, models, taus,\n",
    "                                  title=\"\", y_label_left=\"\", y_label_right=\"\",\n",
    "                                  include_std=False, grid=True):\n",
    "    \"\"\"\n",
    "    Generate more advanced PGFPlots code with better styling options.\n",
    "    \"\"\"\n",
    "    \n",
    "    colors = {\n",
    "        'SQR': 'red',\n",
    "        'LightGBM': 'blue',\n",
    "        'DecisionTree': 'green',\n",
    "        'LinearQuantile': 'purple'\n",
    "    }\n",
    "    \n",
    "    styles = {\n",
    "        'SQR': 'solid',\n",
    "        'LightGBM': 'dashed',\n",
    "        'DecisionTree': 'dotted',\n",
    "        'LinearQuantile': 'dashdotted'\n",
    "    }\n",
    "    \n",
    "    code = r\"\\begin{tikzpicture}[scale=1.0]\" + \"\\n\"\n",
    "    code += r\"\\begin{groupplot}[\" + \"\\n\"\n",
    "    code += f\"    group style={{group size=1 by 1}},\\n\"\n",
    "    code += f\"    title={{{title}}},\\n\"\n",
    "    code += f\"]\\n\"\n",
    "    code += r\"\\nextgroupplot[\" + \"\\n\"\n",
    "    code += f\"    xlabel={{$\\\\tau$}},\\n\"\n",
    "    code += f\"    ylabel={{{y_label_left}}},\\n\"\n",
    "    code += f\"    width=12cm,\\n\"\n",
    "    code += f\"    height=6cm,\\n\"\n",
    "    code += f\"    legend pos=outer north east,\\n\"\n",
    "    code += f\"    legend columns=2,\\n\"\n",
    "    code += f\"    xtick={{{','.join(map(str, taus))}}},\\n\"\n",
    "    \n",
    "    if grid:\n",
    "        code += f\"    grid=major,\\n\"\n",
    "    \n",
    "    code += f\"]\\n\"\n",
    "    \n",
    "    # Left axis plots\n",
    "    for metric in left_metrics:\n",
    "        if metric not in metric_data:\n",
    "            continue\n",
    "        metric_df = metric_data[metric]\n",
    "        \n",
    "        for model in models:\n",
    "            model_subset = metric_df[metric_df['model'] == model].sort_values('tau')\n",
    "            \n",
    "            if len(model_subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            x_vals = model_subset['tau'].tolist()\n",
    "            y_vals = model_subset['mean'].tolist()\n",
    "            \n",
    "            coords = ', '.join([f\"({x:.1f},{y:.4f})\" for x, y in zip(x_vals, y_vals)])\n",
    "            \n",
    "            code += f\"\\\\addplot[color={colors.get(model, 'black')}, \"\n",
    "            code += f\"{styles.get(model, 'solid')}, thick, \"\n",
    "            code += f\"mark=*, mark size=3pt] coordinates {{{coords}}};\\n\"\n",
    "            code += f\"\\\\addlegendentry{{{model}}}\\n\"\n",
    "    \n",
    "    code += r\"\\end{groupplot}\" + \"\\n\"\n",
    "    code += r\"\\end{tikzpicture}\" + \"\\n\"\n",
    "    \n",
    "    return code\n",
    "\n",
    "\n",
    "# Generate advanced plots\n",
    "advanced_config = {\n",
    "    'name': 'advanced_performance',\n",
    "    'left_metrics': ['losses'],\n",
    "    'right_metrics': [],\n",
    "    'title': 'Normalized Pinball Loss vs Quantile Level (τ)',\n",
    "    'y_label_left': 'Normalized Loss',\n",
    "    'y_label_right': ''\n",
    "}\n",
    "\n",
    "advanced_tikz = generate_pgfplots_dual_axis(\n",
    "    metric_data,\n",
    "    left_metrics=advanced_config['left_metrics'],\n",
    "    right_metrics=advanced_config['right_metrics'],\n",
    "    models=models,\n",
    "    taus=taus,\n",
    "    title=advanced_config['title'],\n",
    "    y_label_left=advanced_config['y_label_left'],\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "save_tikz_standalone(advanced_tikz, 'pgfplots_losses')\n",
    "\n",
    "print(\"✓ Generated advanced PGFPlots visualization\")\n",
    "print(f\"  File: {OUTPUT_DIR}/pgfplots_losses.tikz\")\n",
    "\n",
    "# Create a comparison table showing tau effect on each model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOSS TREND ANALYSIS ACROSS TAU\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "losses_pivot = summary_stats[summary_stats['metric'] == 'losses'].pivot_table(\n",
    "    index='model',\n",
    "    columns='tau',\n",
    "    values='mean'\n",
    ")\n",
    "\n",
    "print(\"\\nNormalized Pinball Loss by Model and Tau:\")\n",
    "print(losses_pivot.round(4))\n",
    "\n",
    "# Calculate trend (improvement/degradation)\n",
    "if len(taus) > 1:\n",
    "    trend = pd.DataFrame()\n",
    "    trend['tau_min'] = losses_pivot[taus[0]]\n",
    "    trend['tau_max'] = losses_pivot[taus[-1]]\n",
    "    trend['delta'] = trend['tau_max'] - trend['tau_min']\n",
    "    trend['percent_change'] = (trend['delta'] / trend['tau_min'] * 100).round(2)\n",
    "    \n",
    "    print(\"\\nTrend Analysis (from τ=\" + str(taus[0]) + \" to τ=\" + str(taus[-1]) + \"):\")\n",
    "    print(trend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777c61a",
   "metadata": {},
   "source": [
    "## Advanced: Custom Plot Generation\n",
    "Generate custom TikZ plots with specific metric combinations and styling options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614eae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY STATISTICS ACROSS TAU LEVELS\n",
      "================================================================================\n",
      "\n",
      "--- TAU = 0.5 ---\n",
      "\n",
      "LOSSES:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.038717 0.017254\n",
      "      LightGBM 0.042091 0.021733\n",
      "LinearQuantile 0.057968 0.029411\n",
      "           SQR 0.032786 0.023446\n",
      "\n",
      "COVERAGE:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.083617 0.099913\n",
      "      LightGBM 0.058731 0.062022\n",
      "LinearQuantile 0.059011 0.073643\n",
      "           SQR 0.081545 0.086229\n",
      "\n",
      "TIME_ALL:\n",
      "         model       mean        std\n",
      "  DecisionTree   7.856714  47.333635\n",
      "      LightGBM   0.603711   1.052114\n",
      "LinearQuantile  71.279094 495.958995\n",
      "           SQR 165.855329 170.784553\n",
      "\n",
      "--- TAU = 0.6 ---\n",
      "\n",
      "LOSSES:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.038134 0.017316\n",
      "      LightGBM 0.043783 0.029528\n",
      "LinearQuantile 0.058978 0.032384\n",
      "           SQR 0.031723 0.022169\n",
      "\n",
      "COVERAGE:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.082674 0.082266\n",
      "      LightGBM 0.062003 0.070576\n",
      "LinearQuantile 0.106894 0.082974\n",
      "           SQR 0.079169 0.080322\n",
      "\n",
      "TIME_ALL:\n",
      "         model       mean        std\n",
      "  DecisionTree   7.842902  47.136741\n",
      "      LightGBM   0.612140   1.125557\n",
      "LinearQuantile  26.663235 128.743428\n",
      "           SQR 164.835667 169.508130\n",
      "\n",
      "--- TAU = 0.7 ---\n",
      "\n",
      "LOSSES:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.035276 0.016283\n",
      "      LightGBM 0.041879 0.027761\n",
      "LinearQuantile 0.058438 0.035775\n",
      "           SQR 0.029593 0.022415\n",
      "\n",
      "COVERAGE:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.087250 0.068894\n",
      "      LightGBM 0.059984 0.066836\n",
      "LinearQuantile 0.162340 0.117542\n",
      "           SQR 0.073303 0.077523\n",
      "\n",
      "TIME_ALL:\n",
      "         model       mean        std\n",
      "  DecisionTree   7.759449  46.919006\n",
      "      LightGBM   0.606054   1.107590\n",
      "LinearQuantile  28.914503 150.906111\n",
      "           SQR 165.321588 166.520335\n",
      "\n",
      "--- TAU = 0.8 ---\n",
      "\n",
      "LOSSES:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.029495 0.013844\n",
      "      LightGBM 0.036403 0.022113\n",
      "LinearQuantile 0.056426 0.038815\n",
      "           SQR 0.027207 0.063031\n",
      "\n",
      "COVERAGE:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.083559 0.063142\n",
      "      LightGBM 0.056286 0.055699\n",
      "LinearQuantile 0.218424 0.157960\n",
      "           SQR 0.067302 0.075860\n",
      "\n",
      "TIME_ALL:\n",
      "         model       mean        std\n",
      "  DecisionTree   8.111070  48.436527\n",
      "      LightGBM   0.568040   1.083882\n",
      "LinearQuantile  28.327441 143.642962\n",
      "           SQR 165.804617 167.065665\n",
      "\n",
      "--- TAU = 0.9 ---\n",
      "\n",
      "LOSSES:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.019622 0.009453\n",
      "      LightGBM 0.024369 0.012036\n",
      "LinearQuantile 0.052025 0.043581\n",
      "           SQR      inf      NaN\n",
      "\n",
      "COVERAGE:\n",
      "         model     mean      std\n",
      "  DecisionTree 0.063960 0.057168\n",
      "      LightGBM 0.038269 0.037285\n",
      "LinearQuantile 0.272719 0.210919\n",
      "           SQR 0.059893 0.075247\n",
      "\n",
      "TIME_ALL:\n",
      "         model       mean        std\n",
      "  DecisionTree   7.804577  46.911510\n",
      "      LightGBM   0.532874   1.073291\n",
      "LinearQuantile  46.197629 326.500703\n",
      "           SQR 166.141980 168.749838\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL BY METRIC AND TAU\n",
      "================================================================================\n",
      "\n",
      "LOSSES:\n",
      "  τ=0.5: SQR             (mean=0.0328)\n",
      "  τ=0.6: SQR             (mean=0.0317)\n",
      "  τ=0.7: SQR             (mean=0.0296)\n",
      "  τ=0.8: SQR             (mean=0.0272)\n",
      "  τ=0.9: DecisionTree    (mean=0.0196)\n",
      "\n",
      "COVERAGE:\n",
      "  τ=0.5: LightGBM        (mean=0.0587)\n",
      "  τ=0.6: LightGBM        (mean=0.0620)\n",
      "  τ=0.7: LightGBM        (mean=0.0600)\n",
      "  τ=0.8: LightGBM        (mean=0.0563)\n",
      "  τ=0.9: LightGBM        (mean=0.0383)\n",
      "\n",
      "✓ Exported summary statistics to: tikz_plots/summary_statistics.csv\n",
      "✓ Exported model rankings to: tikz_plots/model_rankings.csv\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "TikZ plots generated in: tikz_plots/\n",
      "Supporting files:\n",
      "  - tikz_plots/summary_statistics.csv\n",
      "  - tikz_plots/model_rankings.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics table\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS ACROSS TAU LEVELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for tau in taus:\n",
    "    print(f\"\\n--- TAU = {tau} ---\")\n",
    "    tau_data = summary_stats[summary_stats['tau'] == tau]\n",
    "    \n",
    "    for metric in ['losses', 'coverage', 'time_all']:\n",
    "        if metric not in tau_data['metric'].values:\n",
    "            continue\n",
    "        \n",
    "        metric_data_tau = tau_data[tau_data['metric'] == metric][['model', 'mean', 'std']]\n",
    "        if len(metric_data_tau) > 0:\n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            print(metric_data_tau.to_string(index=False))\n",
    "\n",
    "# Find best and worst performing models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL BY METRIC AND TAU\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric in ['losses', 'coverage']:\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    metric_summary = summary_stats[summary_stats['metric'] == metric]\n",
    "    \n",
    "    for tau in taus:\n",
    "        tau_metric = metric_summary[metric_summary['tau'] == tau]\n",
    "        best_idx = tau_metric['mean'].idxmin()\n",
    "        best_model = tau_metric.loc[best_idx, 'model']\n",
    "        best_value = tau_metric.loc[best_idx, 'mean']\n",
    "        print(f\"  τ={tau}: {best_model:15s} (mean={best_value:.4f})\")\n",
    "\n",
    "# Export summary table to CSV\n",
    "summary_csv_path = os.path.join(OUTPUT_DIR, 'summary_statistics.csv')\n",
    "summary_stats.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\n✓ Exported summary statistics to: {summary_csv_path}\")\n",
    "\n",
    "# Export model rankings\n",
    "rankings_path = os.path.join(OUTPUT_DIR, 'model_rankings.csv')\n",
    "ranking_data = []\n",
    "\n",
    "for tau in taus:\n",
    "    for metric in ['losses', 'coverage', 'time_all']:\n",
    "        metric_data_tau = summary_stats[\n",
    "            (summary_stats['tau'] == tau) & \n",
    "            (summary_stats['metric'] == metric)\n",
    "        ].sort_values('mean')\n",
    "        \n",
    "        for rank, (idx, row) in enumerate(metric_data_tau.iterrows(), 1):\n",
    "            ranking_data.append({\n",
    "                'tau': tau,\n",
    "                'metric': metric,\n",
    "                'rank': rank,\n",
    "                'model': row['model'],\n",
    "                'mean': row['mean'],\n",
    "                'std': row['std']\n",
    "            })\n",
    "\n",
    "rankings_df = pd.DataFrame(ranking_data)\n",
    "rankings_df.to_csv(rankings_path, index=False)\n",
    "print(f\"✓ Exported model rankings to: {rankings_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"TikZ plots generated in: {OUTPUT_DIR}\")\n",
    "print(f\"Supporting files:\")\n",
    "print(f\"  - {summary_csv_path}\")\n",
    "print(f\"  - {rankings_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0f9f6",
   "metadata": {},
   "source": [
    "## Summary: Results Overview\n",
    "Display key statistics and insights from the tau analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e603a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TikZ plot: tikz_plots/performance_vs_time.tikz\n",
      "Saved standalone LaTeX: tikz_plots/performance_vs_time_standalone.tex\n",
      "Saved TikZ plot: tikz_plots/complexity_vs_time.tikz\n",
      "Saved standalone LaTeX: tikz_plots/complexity_vs_time_standalone.tex\n",
      "Saved TikZ plot: tikz_plots/all_metrics.tikz\n",
      "Saved standalone LaTeX: tikz_plots/all_metrics_standalone.tex\n",
      "\n",
      "Generated 3 TikZ plots in tikz_plots//\n",
      "\n",
      "To use in LaTeX, add to your document:\n",
      "  \\input{tikz_plots/performance_vs_time.tikz}\n"
     ]
    }
   ],
   "source": [
    "def save_tikz_standalone(tikz_code, filename, directory='tikz_plots/'):\n",
    "    \"\"\"\n",
    "    Save TikZ code as a standalone file that can be included in LaTeX.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tikz_code : str\n",
    "        TikZ code to save\n",
    "    filename : str\n",
    "        Name of output file (without extension)\n",
    "    directory : str\n",
    "        Output directory\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Option 1: Save as raw TikZ for use with \\\\input{}\n",
    "    tikz_path = os.path.join(directory, f\"{filename}.tikz\")\n",
    "    with open(tikz_path, 'w') as f:\n",
    "        f.write(tikz_code)\n",
    "    print(f\"Saved TikZ plot: {tikz_path}\")\n",
    "    \n",
    "    # Option 2: Save as standalone LaTeX document\n",
    "    standalone_path = os.path.join(directory, f\"{filename}_standalone.tex\")\n",
    "    standalone_doc = r\"\"\"\\documentclass[tikz,border=10pt]{standalone}\n",
    "\\usepackage{tikz}\n",
    "\\usepackage{pgfplots}\n",
    "\\pgfplotsset{compat=1.16}\n",
    "\\usepgfplotslibrary{groupplots}\n",
    "\n",
    "\\begin{document}\n",
    "\"\"\" + tikz_code + r\"\"\"\n",
    "\\end{document}\"\"\"\n",
    "    \n",
    "    with open(standalone_path, 'w') as f:\n",
    "        f.write(standalone_doc)\n",
    "    print(f\"Saved standalone LaTeX: {standalone_path}\")\n",
    "\n",
    "# Generate and save multiple plot configurations\n",
    "plot_configs = [\n",
    "    {\n",
    "        'name': 'performance_vs_time',\n",
    "        'left_metrics': ['losses', 'coverage'],\n",
    "        'right_metrics': ['time_all'],\n",
    "        'title': 'Model Performance vs Computation Time Across Quantile Levels',\n",
    "        'y_label_left': 'Normalized Loss / Coverage Error',\n",
    "        'y_label_right': 'Total Time (s)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'complexity_vs_time',\n",
    "        'left_metrics': ['complexity'],\n",
    "        'right_metrics': ['time_fit'],\n",
    "        'title': 'Model Complexity vs Fitting Time Across Quantile Levels',\n",
    "        'y_label_left': 'Expression/Tree Complexity',\n",
    "        'y_label_right': 'Fitting Time (s)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'all_metrics',\n",
    "        'left_metrics': ['losses'],\n",
    "        'right_metrics': ['time_all'],\n",
    "        'title': 'All Metrics Across Quantile Levels',\n",
    "        'y_label_left': 'Loss (Normalized Pinball)',\n",
    "        'y_label_right': 'Computation Time (s)'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate plots for each configuration\n",
    "for config in plot_configs:\n",
    "    tikz_code = generate_tikz_dual_axis_plot(\n",
    "        metric_data,\n",
    "        left_metrics=config['left_metrics'],\n",
    "        right_metrics=config['right_metrics'],\n",
    "        models=models,\n",
    "        taus=taus,\n",
    "        title=config['title'],\n",
    "        y_label_left=config['y_label_left'],\n",
    "        y_label_right=config['y_label_right']\n",
    "    )\n",
    "    save_tikz_standalone(tikz_code, config['name'])\n",
    "\n",
    "print(f\"\\nGenerated {len(plot_configs)} TikZ plots in {OUTPUT_DIR}/\")\n",
    "print(\"\\nTo use in LaTeX, add to your document:\")\n",
    "print(r\"  \\input{tikz_plots/performance_vs_time.tikz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38220d01",
   "metadata": {},
   "source": [
    "## Section 5: Export TikZ Figures for LaTeX\n",
    "Generate standalone TikZ files that can be included in LaTeX documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d70d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated TikZ code (first 500 chars):\n",
      "\\begin{tikzpicture}\n",
      "\\begin{axis}[\n",
      "    title={Model Performance vs Tau},\n",
      "    xlabel={$\\tau$},\n",
      "    ylabel={Normalized Pinball Loss},\n",
      "    width=0.9\\textwidth,\n",
      "    height=6cm,\n",
      "    legend pos=outer north east,\n",
      "    xtick={0.5,0.6,0.7,0.8,0.9},\n",
      "]\n",
      "\\addplot[color=red, solid, mark=*, mark size=2pt] coordinates {(0.5,0.0328) (0.6,0.0317) (0.7,0.0296) (0.8,0.0272) (0.9,inf)};\n",
      "\\addlegendentry{SQR - losses}\n",
      "\\addplot[color=blue, dashed, mark=o, mark size=2pt] coordinates {(0.5,0.0421) (0.6,0.0438) (0.7,0.0419)\n",
      "\n",
      "Total length: 1896 characters\n"
     ]
    }
   ],
   "source": [
    "def generate_tikz_dual_axis_plot(metric_data, left_metrics, right_metrics, models, taus, \n",
    "                                  title=\"\", y_label_left=\"\", y_label_right=\"\"):\n",
    "    \"\"\"\n",
    "    Generate TikZ code for a dual-axis plot.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metric_data : dict\n",
    "        Dictionary with metric names as keys and DataFrames as values\n",
    "    left_metrics : list\n",
    "        Metrics to plot on left y-axis\n",
    "    right_metrics : list\n",
    "        Metrics to plot on right y-axis\n",
    "    models : list\n",
    "        Model names to include\n",
    "    taus : list\n",
    "        Tau values\n",
    "    title, y_label_left, y_label_right : str\n",
    "        Labels for the plot\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : TikZ code as string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define line styles for models\n",
    "    line_styles = {\n",
    "        'SQR': 'solid',\n",
    "        'LightGBM': 'dashed',\n",
    "        'DecisionTree': 'dotted',\n",
    "        'LinearQuantile': 'dashdotted'\n",
    "    }\n",
    "    \n",
    "    mark_styles = {\n",
    "        'SQR': '*',\n",
    "        'LightGBM': 'o',\n",
    "        'DecisionTree': 'square',\n",
    "        'LinearQuantile': 'diamond'\n",
    "    }\n",
    "    \n",
    "    colors = {\n",
    "        'SQR': 'red',\n",
    "        'LightGBM': 'blue',\n",
    "        'DecisionTree': 'green',\n",
    "        'LinearQuantile': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Start TikZ code\n",
    "    tikz_code = r\"\\begin{tikzpicture}\" + \"\\n\"\n",
    "    tikz_code += r\"\\begin{axis}[\" + \"\\n\"\n",
    "    tikz_code += f\"    title={{{title}}},\\n\"\n",
    "    tikz_code += f\"    xlabel={{$\\\\tau$}},\\n\"\n",
    "    tikz_code += f\"    ylabel={{{y_label_left}}},\\n\"\n",
    "    tikz_code += f\"    width=0.9\\\\textwidth,\\n\"\n",
    "    tikz_code += f\"    height=6cm,\\n\"\n",
    "    tikz_code += f\"    legend pos=outer north east,\\n\"\n",
    "    tikz_code += f\"    xtick={{{','.join(map(str, taus))}}},\\n\"\n",
    "    tikz_code += f\"]\\n\"\n",
    "    \n",
    "    # Plot left axis metrics\n",
    "    for metric in left_metrics:\n",
    "        if metric not in metric_data:\n",
    "            continue\n",
    "        metric_df = metric_data[metric]\n",
    "        \n",
    "        for model in models:\n",
    "            model_data_subset = metric_df[metric_df['model'] == model].sort_values('tau')\n",
    "            \n",
    "            if len(model_data_subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract data\n",
    "            x_vals = model_data_subset['tau'].tolist()\n",
    "            y_vals = model_data_subset['mean'].tolist()\n",
    "            \n",
    "            # Create coordinate string\n",
    "            coords = ' '.join([f\"({x},{y:.4f})\" for x, y in zip(x_vals, y_vals)])\n",
    "            \n",
    "            # Add plot\n",
    "            tikz_code += f\"\\\\addplot[color={colors.get(model, 'black')}, \"\n",
    "            tikz_code += f\"{line_styles.get(model, 'solid')}, \"\n",
    "            tikz_code += f\"mark={mark_styles.get(model, 'o')}, \"\n",
    "            tikz_code += f\"mark size=2pt] coordinates {{{coords}}};\\n\"\n",
    "            tikz_code += f\"\\\\addlegendentry{{{model} - {metric}}}\\n\"\n",
    "    \n",
    "    tikz_code += r\"\\end{axis}\" + \"\\n\"\n",
    "    \n",
    "    # Right axis if needed\n",
    "    if right_metrics:\n",
    "        tikz_code += r\"\\begin{axis}[\" + \"\\n\"\n",
    "        tikz_code += f\"    at=(current axis.right of origin),\\n\"\n",
    "        tikz_code += f\"    axis y line=right,\\n\"\n",
    "        tikz_code += f\"    ylabel={{{y_label_right}}},\\n\"\n",
    "        tikz_code += f\"    legend pos=outer north east,\\n\"\n",
    "        tikz_code += f\"]\\n\"\n",
    "        \n",
    "        for metric in right_metrics:\n",
    "            if metric not in metric_data:\n",
    "                continue\n",
    "            metric_df = metric_data[metric]\n",
    "            \n",
    "            for model in models:\n",
    "                model_data_subset = metric_df[metric_df['model'] == model].sort_values('tau')\n",
    "                \n",
    "                if len(model_data_subset) == 0:\n",
    "                    continue\n",
    "                \n",
    "                x_vals = model_data_subset['tau'].tolist()\n",
    "                y_vals = model_data_subset['mean'].tolist()\n",
    "                coords = ' '.join([f\"({x},{y:.4f})\" for x, y in zip(x_vals, y_vals)])\n",
    "                \n",
    "                # Add plot with different dash pattern to distinguish from left axis\n",
    "                tikz_code += f\"\\\\addplot[color={colors.get(model, 'black')}, \"\n",
    "                tikz_code += f\"{line_styles.get(model, 'solid')}, \"\n",
    "                tikz_code += f\"mark={mark_styles.get(model, 'o')}, \"\n",
    "                tikz_code += f\"mark size=2pt, line width=1.5pt] coordinates {{{coords}}};\\n\"\n",
    "                tikz_code += f\"\\\\addlegendentry{{{model} - {metric}}}\\n\"\n",
    "        \n",
    "        tikz_code += r\"\\end{axis}\" + \"\\n\"\n",
    "    \n",
    "    tikz_code += r\"\\end{tikzpicture}\" + \"\\n\"\n",
    "    \n",
    "    return tikz_code\n",
    "\n",
    "# Test the function with a simple example\n",
    "test_tikz = generate_tikz_dual_axis_plot(\n",
    "    metric_data,\n",
    "    left_metrics=['losses'],\n",
    "    right_metrics=['time_all'],\n",
    "    models=['SQR', 'LightGBM', 'DecisionTree', 'LinearQuantile'],\n",
    "    taus=taus,\n",
    "    title=\"Model Performance vs Tau\",\n",
    "    y_label_left=\"Normalized Pinball Loss\",\n",
    "    y_label_right=\"Computation Time (s)\"\n",
    ")\n",
    "\n",
    "print(\"Generated TikZ code (first 500 chars):\")\n",
    "print(test_tikz[:500])\n",
    "print(f\"\\nTotal length: {len(test_tikz)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df43737",
   "metadata": {},
   "source": [
    "## Section 4: Generate TikZ Plots with Dual Axes\n",
    "Create TikZ plot objects with dual y-axes (left for one metric group, right for another).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bb87d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['DecisionTree', 'LightGBM', 'LinearQuantile', 'SQR']\n",
      "Metrics: ['complexity', 'coverage', 'losses', 'time_all', 'time_fit']\n",
      "Tau values: [np.float64(0.5), np.float64(0.6), np.float64(0.7), np.float64(0.8), np.float64(0.9)]\n",
      "\n",
      "Example: Losses across tau levels\n",
      "    tau           model      mean       std\n",
      "2   0.5    DecisionTree  0.038717  0.017254\n",
      "6   0.5        LightGBM  0.042091  0.021733\n",
      "11  0.5  LinearQuantile  0.057968  0.029411\n",
      "16  0.5             SQR  0.032786  0.023446\n",
      "21  0.6    DecisionTree  0.038134  0.017316\n",
      "25  0.6        LightGBM  0.043783  0.029528\n",
      "30  0.6  LinearQuantile  0.058978  0.032384\n",
      "35  0.6             SQR  0.031723  0.022169\n",
      "54  0.7             SQR  0.029593  0.022415\n",
      "49  0.7  LinearQuantile  0.058438  0.035775\n",
      "44  0.7        LightGBM  0.041879  0.027761\n",
      "40  0.7    DecisionTree  0.035276  0.016283\n",
      "59  0.8    DecisionTree  0.029495  0.013844\n",
      "63  0.8        LightGBM  0.036403  0.022113\n",
      "68  0.8  LinearQuantile  0.056426  0.038815\n",
      "73  0.8             SQR  0.027207  0.063031\n",
      "78  0.9    DecisionTree  0.019622  0.009453\n",
      "82  0.9        LightGBM  0.024369  0.012036\n",
      "87  0.9  LinearQuantile  0.052025  0.043581\n",
      "92  0.9             SQR       inf       NaN\n"
     ]
    }
   ],
   "source": [
    "# Define metric groupings for dual-axis plots\n",
    "# Typical arrangement: performance metrics (loss, coverage) on left; complexity/time on right\n",
    "metric_groups = {\n",
    "    'Performance': {\n",
    "        'left': ['losses', 'coverage'],\n",
    "        'right': ['time_all']\n",
    "    },\n",
    "    'Complexity': {\n",
    "        'left': ['complexity'],\n",
    "        'right': ['time_fit']\n",
    "    },\n",
    "    'All Metrics': {\n",
    "        'left': ['losses', 'coverage', 'complexity'],\n",
    "        'right': ['time_all', 'time_fit']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create tau-ordered data for each model and metric\n",
    "models = sorted(df_long['model'].unique())\n",
    "metrics = sorted(df_long['metric'].unique())\n",
    "taus = sorted(df_long['tau'].unique())\n",
    "\n",
    "print(f\"Models: {models}\")\n",
    "print(f\"Metrics: {metrics}\")\n",
    "print(f\"Tau values: {taus}\")\n",
    "\n",
    "# Create a dictionary of DataFrames organized by metric for easy access\n",
    "metric_data = {}\n",
    "for metric in metrics:\n",
    "    metric_subset = summary_stats[summary_stats['metric'] == metric].copy()\n",
    "    metric_subset = metric_subset.sort_values('tau')\n",
    "    metric_data[metric] = metric_subset\n",
    "\n",
    "# Display example data for losses metric\n",
    "print(\"\\nExample: Losses across tau levels\")\n",
    "print(metric_data['losses'][['tau', 'model', 'mean', 'std']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95713dd1",
   "metadata": {},
   "source": [
    "## Section 3: Prepare Data for Plotting\n",
    "Reshape aggregated data and organize metrics into groups for dual-axis visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "481d0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics shape: (95, 9)\n",
      "\n",
      "Sample statistics:\n",
      "    tau           model      metric         mean          std     median  \\\n",
      "0   0.5    DecisionTree  complexity  1526.940000  9135.109392  79.000000   \n",
      "1   0.5    DecisionTree    coverage     0.083617     0.099913   0.046872   \n",
      "2   0.5    DecisionTree      losses     0.038717     0.017254   0.038650   \n",
      "3   0.5    DecisionTree    time_all     7.856714    47.333635   0.096175   \n",
      "4   0.5    DecisionTree    time_fit     0.526921     3.229699   0.014585   \n",
      "5   0.5        LightGBM    coverage     0.058731     0.062022   0.040000   \n",
      "6   0.5        LightGBM      losses     0.042091     0.021733   0.036913   \n",
      "7   0.5        LightGBM    time_all     0.603711     1.052114   0.255252   \n",
      "8   0.5        LightGBM    time_fit     0.117287     0.215799   0.053976   \n",
      "9   0.5  LinearQuantile  complexity    19.891667    20.600256  11.000000   \n",
      "10  0.5  LinearQuantile    coverage     0.059011     0.073643   0.040000   \n",
      "11  0.5  LinearQuantile      losses     0.057968     0.029411   0.058179   \n",
      "12  0.5  LinearQuantile    time_all    71.279094   495.958995   0.732135   \n",
      "13  0.5  LinearQuantile    time_fit     6.012549    48.475666   0.058632   \n",
      "14  0.5             SQR  complexity    10.101667     4.500936  11.000000   \n",
      "\n",
      "             min            max  count  \n",
      "0   1.000000e+00  128943.000000    600  \n",
      "1   0.000000e+00       0.446226    600  \n",
      "2   9.759806e-04       0.107143    600  \n",
      "3   2.416039e-02     412.072100    600  \n",
      "4   1.440763e-03      31.157656    600  \n",
      "5   0.000000e+00       0.400000    600  \n",
      "6   4.062980e-03       0.150808    600  \n",
      "7   3.056002e-02       7.591570    600  \n",
      "8   3.676414e-03       1.609080    600  \n",
      "9   2.000000e+00     124.000000    600  \n",
      "10  0.000000e+00       0.500000    600  \n",
      "11  4.652457e-14       0.230189    600  \n",
      "12  2.730465e-02    5742.128815    600  \n",
      "13  1.907349e-03     776.256346    600  \n",
      "14  2.000000e+00      21.000000    600  \n",
      "\n",
      "Pivoted statistics (means):\n",
      "metric  tau           model   complexity  coverage    losses    time_all  \\\n",
      "0       0.5    DecisionTree  1526.940000  0.083617  0.038717    7.856714   \n",
      "1       0.5        LightGBM          NaN  0.058731  0.042091    0.603711   \n",
      "2       0.5  LinearQuantile    19.891667  0.059011  0.057968   71.279094   \n",
      "3       0.5             SQR    10.101667  0.081545  0.032786  165.855329   \n",
      "4       0.6    DecisionTree  1307.157983  0.082674  0.038134    7.842902   \n",
      "5       0.6        LightGBM          NaN  0.062003  0.043783    0.612140   \n",
      "6       0.6  LinearQuantile    19.941176  0.106894  0.058978   26.663235   \n",
      "7       0.6             SQR     9.843697  0.079169  0.031723  164.835667   \n",
      "8       0.7    DecisionTree  1050.726667  0.087250  0.035276    7.759449   \n",
      "9       0.7        LightGBM          NaN  0.059984  0.041879    0.606054   \n",
      "10      0.7  LinearQuantile    19.891667  0.162340  0.058438   28.914503   \n",
      "11      0.7             SQR     9.598333  0.073303  0.029593  165.321588   \n",
      "12      0.8    DecisionTree   976.816667  0.083559  0.029495    8.111070   \n",
      "13      0.8        LightGBM          NaN  0.056286  0.036403    0.568040   \n",
      "14      0.8  LinearQuantile    19.891667  0.218424  0.056426   28.327441   \n",
      "15      0.8             SQR     9.735000  0.067302  0.027207  165.804617   \n",
      "16      0.9    DecisionTree   938.870000  0.063960  0.019622    7.804577   \n",
      "17      0.9        LightGBM          NaN  0.038269  0.024369    0.532874   \n",
      "18      0.9  LinearQuantile    19.891667  0.272719  0.052025   46.197629   \n",
      "19      0.9             SQR     9.678333  0.059893       inf  166.141980   \n",
      "\n",
      "metric    time_fit  \n",
      "0         0.526921  \n",
      "1         0.117287  \n",
      "2         6.012549  \n",
      "3       165.855329  \n",
      "4         0.484528  \n",
      "5         0.119900  \n",
      "6         2.438704  \n",
      "7       164.835667  \n",
      "8         0.429903  \n",
      "9         0.117840  \n",
      "10        2.612462  \n",
      "11      165.321588  \n",
      "12        0.439787  \n",
      "13        0.113884  \n",
      "14        2.476702  \n",
      "15      165.804617  \n",
      "16        0.422117  \n",
      "17        0.101827  \n",
      "18        4.217849  \n",
      "19      166.141980  \n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics grouped by tau, model, and metric\n",
    "# Compute across all datasets and runs for each combination\n",
    "summary_stats = df_long.groupby(['tau', 'model', 'metric'])['value'].agg([\n",
    "    ('mean', 'mean'),\n",
    "    ('std', 'std'),\n",
    "    ('median', 'median'),\n",
    "    ('min', 'min'),\n",
    "    ('max', 'max'),\n",
    "    ('count', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "print(\"Summary statistics shape:\", summary_stats.shape)\n",
    "print(\"\\nSample statistics:\")\n",
    "print(summary_stats.head(15))\n",
    "\n",
    "# Pivot to get metrics as columns for easier access\n",
    "stats_pivot = summary_stats.pivot_table(\n",
    "    index=['tau', 'model'],\n",
    "    columns='metric',\n",
    "    values='mean'\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nPivoted statistics (means):\")\n",
    "print(stats_pivot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df2a29",
   "metadata": {},
   "source": [
    "## Section 2: Calculate Summary Statistics\n",
    "Group data by tau, model, and metric to compute mean, median, and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7881dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading results0.6_503_wind_tau0.6.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loaded 56905 records from results_sampled_10k_taus/\n",
      "Unique tau values: [np.float64(0.5), np.float64(0.6), np.float64(0.7), np.float64(0.8), np.float64(0.9)]\n",
      "Unique models: ['DecisionTree', 'LightGBM', 'LinearQuantile', 'SQR']\n",
      "Unique metrics: ['complexity', 'coverage', 'losses', 'time_all', 'time_fit']\n",
      "\n",
      "DataFrame shape: (56905, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>model</th>\n",
       "      <th>tau</th>\n",
       "      <th>metric</th>\n",
       "      <th>dataset</th>\n",
       "      <th>run</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>losses</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.367347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.112245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.283505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>results0.5_1027_ESL_tau0.5.json</td>\n",
       "      <td>SQR</td>\n",
       "      <td>0.5</td>\n",
       "      <td>coverage</td>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.273196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename model  tau    metric   dataset  run  \\\n",
       "0  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    0   \n",
       "1  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    1   \n",
       "2  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    2   \n",
       "3  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    3   \n",
       "4  results0.5_1027_ESL_tau0.5.json   SQR  0.5    losses  1027_ESL    4   \n",
       "5  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    0   \n",
       "6  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    1   \n",
       "7  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    2   \n",
       "8  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    3   \n",
       "9  results0.5_1027_ESL_tau0.5.json   SQR  0.5  coverage  1027_ESL    4   \n",
       "\n",
       "      value  \n",
       "0  0.035077  \n",
       "1  0.044006  \n",
       "2  0.034439  \n",
       "3  0.034149  \n",
       "4  0.038015  \n",
       "5  0.367347  \n",
       "6  0.112245  \n",
       "7  0.397959  \n",
       "8  0.283505  \n",
       "9  0.273196  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tau_jsons_to_long_df(directory):\n",
    "    \"\"\"Load JSON files from tau results directory into long-format DataFrame.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                for model, model_data in data.items():\n",
    "                    tau = model_data.get('tau')\n",
    "                    for metric, metric_data in model_data.items():\n",
    "                        if metric == 'tau' or metric == 'sizes':\n",
    "                            continue\n",
    "                        for dataset, values in metric_data.items():\n",
    "                            for run_idx, value in enumerate(values):\n",
    "                                records.append({\n",
    "                                    'filename': filename,\n",
    "                                    'model': model,\n",
    "                                    'tau': tau,\n",
    "                                    'metric': metric,\n",
    "                                    'dataset': dataset,\n",
    "                                    'run': run_idx,\n",
    "                                    'value': value\n",
    "                                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    return df\n",
    "\n",
    "# Load results\n",
    "df_long = load_tau_jsons_to_long_df(RESULTS_DIR)\n",
    "print(f\"Loaded {len(df_long)} records from {RESULTS_DIR}\")\n",
    "print(f\"Unique tau values: {sorted(df_long['tau'].unique())}\")\n",
    "print(f\"Unique models: {sorted(df_long['model'].unique())}\")\n",
    "print(f\"Unique metrics: {sorted(df_long['metric'].unique())}\")\n",
    "print(f\"\\nDataFrame shape: {df_long.shape}\")\n",
    "df_long.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa9244",
   "metadata": {},
   "source": [
    "## Section 1: Load and Aggregate Results by Tau\n",
    "Load JSON files from results_sampled_10k_taus directory and create a long-format DataFrame with tau as a separate dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0afc1af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: results_sampled_10k_taus/\n",
      "Output directory: tikz_plots/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "RESULTS_DIR = 'results_sampled_10k_taus/'\n",
    "OUTPUT_DIR = 'tikz_plots/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028277f6",
   "metadata": {},
   "source": [
    "# Analysis of SQR Results Across Tau Levels with TikZ Plotting\n",
    "Analysis of `results_sampled_10k_taus` directory with visualization using TikZ for LaTeX embedding. This notebook aggregates results across multiple quantile levels (τ) and generates publication-quality plots with dual y-axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c09b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd356872",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqr-noversion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
